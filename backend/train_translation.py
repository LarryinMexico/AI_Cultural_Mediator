#!/usr/bin/env python3
"""
ä½¿ç”¨ MLX LoRA å¾®èª¿ç¿»è­¯æ¨¡åž‹
"""
import os
import json
import mlx.core as mx
import mlx_lm
from pathlib import Path
import logging
from datetime import datetime
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TrainingLogger:
    """è¨˜éŒ„è¨“ç·´éŽç¨‹ä»¥ä¾¿å¾ŒçºŒè¦–è¦ºåŒ–"""
    def __init__(self, log_dir="training_logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # å‰µå»ºæ™‚é–“æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = self.log_dir / f"training_{timestamp}.jsonl"
        self.summary_file = self.log_dir / f"summary_{timestamp}.json"
        
        self.start_time = time.time()
        self.logs = []
        
        logger.info(f"ðŸ“Š è¨“ç·´æ—¥èªŒå°‡ä¿å­˜åˆ°: {self.log_file}")
    
    def log_step(self, step, loss, learning_rate, **kwargs):
        """è¨˜éŒ„è¨“ç·´æ­¥é©Ÿ"""
        log_entry = {
            "step": step,
            "loss": float(loss),
            "learning_rate": float(learning_rate),
            "timestamp": datetime.now().isoformat(),
            "elapsed_time": time.time() - self.start_time,
            **kwargs
        }
        
        self.logs.append(log_entry)
        
        # å³æ™‚å¯«å…¥
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry) + '\n')
    
    def save_summary(self, config, final_metrics):
        """ä¿å­˜è¨“ç·´æ‘˜è¦"""
        summary = {
            "config": config,
            "final_metrics": final_metrics,
            "total_time": time.time() - self.start_time,
            "num_steps": len(self.logs),
            "start_time": datetime.fromtimestamp(self.start_time).isoformat(),
            "end_time": datetime.now().isoformat()
        }
        
        with open(self.summary_file, 'w', encoding='utf-8') as f:
            json.dumps(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… è¨“ç·´æ‘˜è¦å·²ä¿å­˜: {self.summary_file}")

def train_translation_model():
    """ä½¿ç”¨ MLX LoRA å¾®èª¿æ¨¡åž‹"""
    
    print("=" * 60)
    print("é–‹å§‹å¾®èª¿ç¿»è­¯æ¨¡åž‹")
    print("=" * 60)
    
    # é…ç½®
    config = {
        "model": "Qwen/Qwen2.5-3B-Instruct",
        "data": "training_data_dir",  # ç›®éŒ„è·¯å¾‘
        "adapter_path": "adapters/translation",
        "iters": 1000,  # è¿­ä»£æ¬¡æ•¸ï¼ˆå¯èª¿æ•´ï¼‰
        "learning_rate": 1e-4,
        "steps_per_report": 50,
        "save_every": 100,
        "lora_layers": 16,  # LoRA å±¤æ•¸
        "batch_size": 2,    # æ‰¹æ¬¡å¤§å°ï¼ˆ3B æ¨¡åž‹å»ºè­°ç”¨å°æ‰¹æ¬¡ï¼‰
    }
    
    print("\nðŸ“‹ è¨“ç·´é…ç½®ï¼š")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # æª¢æŸ¥æ•¸æ“šç›®éŒ„
    if not Path(config["data"]).exists():
        print(f"\nâŒ æ‰¾ä¸åˆ°è¨“ç·´æ•¸æ“šç›®éŒ„: {config['data']}")
        print("è«‹å…ˆé‹è¡Œ: python prepare_training_data.py")
        return
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    Path(config["adapter_path"]).parent.mkdir(parents=True, exist_ok=True)
    
    print("\nðŸš€ é–‹å§‹è¨“ç·´...")
    print("â° é è¨ˆæ™‚é–“: 1-2 å°æ™‚ (å–æ±ºæ–¼ç¡¬é«”)")
    print()
    
    try:
        # ä½¿ç”¨ MLX LoRA è¨“ç·´
        # é€™è£¡éœ€è¦ä½¿ç”¨ mlx-lm çš„ CLI æˆ– API
        # ç”±æ–¼ mlx-lm ä¸»è¦é€šéŽ CLI ä½¿ç”¨ï¼Œæˆ‘å€‘ç”¨ subprocess
        import subprocess
        
        cmd = [
            "python", "-m", "mlx_lm", "lora",
            "--model", config["model"],
            "--train",
            "--data", config["data"],
            "--iters", str(config["iters"]),
            "--learning-rate", str(config["learning_rate"]),
            "--num-layers", str(config["lora_layers"]),
            "--batch-size", str(config["batch_size"]),
            "--steps-per-report", str(config["steps_per_report"]),
            "--save-every", str(config["save_every"]),
            "--adapter-path", config["adapter_path"],
        ]
        
        # MLX LoRA ä¸æ”¯æŒ valid-dataï¼Œç§»é™¤æ­¤éƒ¨åˆ†
        # if Path(config["valid_data"]).exists():
        #     cmd.extend(["--valid-data", config["valid_data"]])
        #     cmd.extend(["--val-batches", str(config["val_batches"])])
        
        print("åŸ·è¡Œå‘½ä»¤:")
        print(" ".join(cmd))
        print()
        
        result = subprocess.run(cmd)
        
        if result.returncode == 0:
            print("\n" + "=" * 60)
            print("âœ… è¨“ç·´å®Œæˆï¼")
            print("=" * 60)
            print(f"\nAdapter å·²ä¿å­˜åˆ°: {config['adapter_path']}")
            print("\nä¸‹ä¸€æ­¥ï¼š")
            print("1. æ¸¬è©¦å¾®èª¿å¾Œçš„æ¨¡åž‹")
            print("2. æ•´åˆåˆ°å°ˆæ¡ˆä¸­")
        else:
            print("\nâŒ è¨“ç·´å¤±æ•—")
            
    except Exception as e:
        logger.error(f"è¨“ç·´éŒ¯èª¤: {e}")
        print("\nå¦‚æžœé‡åˆ°å•é¡Œï¼Œå¯ä»¥æ‰‹å‹•é‹è¡Œï¼š")
        print(f"python -m mlx_lm.lora --model {config['model']} --train --data {config['data']} --iters {config['iters']}")

def test_finetuned_model():
    """æ¸¬è©¦å¾®èª¿å¾Œçš„æ¨¡åž‹"""
    print("\næ¸¬è©¦å¾®èª¿å¾Œçš„æ¨¡åž‹...")
    
    adapter_path = "adapters/translation"
    if not Path(adapter_path).exists():
        print(f"âŒ æ‰¾ä¸åˆ° adapter: {adapter_path}")
        return
    
    # è¼‰å…¥æ¨¡åž‹å’Œ adapter
    print("è¼‰å…¥æ¨¡åž‹...")
    model, tokenizer = mlx_lm.load("Qwen/Qwen2.5-3B-Instruct")
    
    # TODO: è¼‰å…¥ adapter
    # é€™éœ€è¦ mlx-lm çš„ adapter è¼‰å…¥åŠŸèƒ½
    
    # æ¸¬è©¦ç¿»è­¯
    test_texts = [
        "Hello, how are you?",
        "Break a leg!",
        "It's raining cats and dogs.",
    ]
    
    for text in test_texts:
        prompt = f"Translate this English text to Traditional Chinese (ç¹é«”ä¸­æ–‡). Output ONLY the translation, nothing else.\n\nEnglish: {text}\nTraditional Chinese (ç¹é«”ä¸­æ–‡):"
        
        response = mlx_lm.generate(model, tokenizer, prompt=prompt, max_tokens=50, verbose=False)
        
        print(f"\nEN: {text}")
        print(f"ZH: {response.strip()}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_finetuned_model()
    else:
        train_translation_model()
